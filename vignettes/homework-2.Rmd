---
title: "homework-2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bis557)
```
## 1 
CASL 2.11 exercises problem number 5: 
Consider the simple regression model with only a scalar x and intercept $y= \beta_0+ \beta_1x$ <br/>
First we have the design matrix <br/>
$$\begin{pmatrix}
1&  x_1\\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n
\end{pmatrix}
$$
And the response vector Y as follows 
$$ \begin{pmatrix}
 y1 \\ y2 \\ \vdots \\ y_n 
\end{pmatrix}$$

For least squares estimator of simple regression model,
we have 
$\hat\beta = (X^{T}X)^{-1}X^TY$ <br/>
Hence we calculate

$$X^TX = \begin{pmatrix}
1&  1 & \dots &1 \\ x_1 & x_2 & \dots & x_n 
\end{pmatrix}
\begin{pmatrix}
1&  x_1\\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n
\end{pmatrix} =  \begin{pmatrix}
n & \sum_{i=1}^{n}x_i \\ 
 \sum_{i=1}^{n}x_i& \sum_{i=1}^{n}x_i^2 
\end{pmatrix}
$$
According to the relationship between adjoint and inverse of a matrix, 
we have 
$$(X^TX)^{-1} = \frac{adj(X^TX)}{|X^TX|} = 
\frac{\begin{pmatrix}
\sum_{i=1}^{n}x_i^2  & - \sum_{i=1}^{n}x_i  \\
- \sum_{i=1}^{n}x_i  & n \\
\end{pmatrix}}{n\sum_{i=1}^{n}x_i^2 -(\sum_{i=1}^{n}x_i)^2}

$$

Now we calculate the last part, 
$$X^TY =\begin{pmatrix}
1&  1 & \dots &1 \\ x_1 & x_2 & \dots & x_n 
\end{pmatrix}\begin{pmatrix}
 y1 \\ y2 \\ \vdots \\ y_n 
\end{pmatrix} = \begin{pmatrix} 
\sum_{i=1}^{n}y_i \\ \sum_{i=1}^{n}x_iy_i
\end{pmatrix} $$

Hence subsitute back, we have 
$$\hat\beta = (X^{T}X)^{-1}X^TY =
\frac{\begin{pmatrix}
\sum_{i=1}^{n}x_i^2  & - \sum_{i=1}^{n}x_i  \\
- \sum_{i=1}^{n}x_i  & n \\
\end{pmatrix}}{n\sum_{i=1}^{n}x_i^2 -(\sum_{i=1}^{n}x_i)^2} \begin{pmatrix} 
\sum_{i=1}^{n}y_i \\ \sum_{i=1}^{n}x_iy_i
\end{pmatrix} 
$$ 
$$=\frac{1}{n\sum_{i=1}^{n}x_i^2}
\begin{pmatrix}
\sum_{i=1}^{n}x_i^2  & - \sum_{i=1}^{n}x_i  \\
- \sum_{i=1}^{n}x_i  & n \\
\end{pmatrix}
\begin{pmatrix} 
\sum_{i=1}^{n}y_i \\ \sum_{i=1}^{n}x_iy_i
\end{pmatrix} 
=\frac{1}{n\sum_{i=1}^{n}x_i^2} 
\begin{pmatrix}
n\bar{y}\sum x_{i}^2-n\bar{x}\sum x_{i}y_{i}\\
-n^2\bar x \bar y + n \sum x_iy_i\\
\end{pmatrix}
$$
Since $\hat{\beta} = (\beta_0,\beta_1)$

We then have $\hat\beta_0 = \bar{y}-\hat{\beta_1}\bar{x}$
and $\hat\beta_1 = \frac{\sum x_iy_i - \bar{x}\bar{y}}{\sum (x_i-\bar{x})^2}$

## 2 Implement a new function fitting the OLS model using gradient descent that calculates the penalty based on the out-of-sample accuracy. Create test code. How does it compare to the OLS model? 

To compare the two model's accuracy, we compare the residuals of both models fitting to the same dataset

```{r error=T}
library(bis557)
library(rsample)
library(palmerpenguins)
data(penguins)
library(foreach)

  grab_resids <- function(form,data,num_iters,v){
  #create cross-validation folds for out-of-sample accuracy
  v=10
  folds<- vfold_cv(data,v=v)
  y_name<- as.character(form)[2]
  resids<-foreach(fold=folds$splits,.combine = c) %do% {
    fit<- lm(form,analysis(fold))
    as.vector(as.matrix(assessment(fold)[,y_name],ncol=1))-as.vector(predict(fit,assessment(fold)))
  }
  
}
form = bill_length_mm ~ .
data= penguins[,-8]

grab_resids(form=form, data = data,num_iters=1000,v=10)  
gradient_descent_loss(form=form, data = data,alpha=0.1,num_iters=1000,v=10)

# The residual from the model adjusted for out-of-sample accuracy is close to that of the OLS gradient descent model, our model works well.
```
