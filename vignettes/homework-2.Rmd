---
title: "homework-2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bis557)
```
## 1 
CASL 2.11 exercises problem number 5: 
Consider the simple regression model with only a scalar x and intercept $y= \beta_0+ \beta_1x$ <br/>
First we have the design matrix <br/>
$$\begin{pmatrix}
1&  x_1\\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n
\end{pmatrix}
$$
And the response vector Y as follows 
$$ \begin{pmatrix}
 y1 \\ y2 \\ \vdots \\ y_n 
\end{pmatrix}$$

For least squares estimator of simple regression model,
we have 
$\hat\beta = (X^{T}X)^{-1}X^TY$ <br/>
Hence we calculate

$$X^TX = \begin{pmatrix}
1&  1 & \dots &1 \\ x_1 & x_2 & \dots & x_n 
\end{pmatrix}
\begin{pmatrix}
1&  x_1\\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n
\end{pmatrix} =  \begin{pmatrix}
n & \sum_{i=1}^{n}x_i \\ 
 \sum_{i=1}^{n}x_i& \sum_{i=1}^{n}x_i^2 
\end{pmatrix}
$$
According to the relationship between adjoint and inverse of a matrix, 
we have 
$$(X^TX)^{-1} = \frac{adj(X^TX)}{|X^TX|} = 
\frac{\begin{pmatrix}
\sum_{i=1}^{n}x_i^2  & - \sum_{i=1}^{n}x_i  \\
- \sum_{i=1}^{n}x_i  & n \\
\end{pmatrix}}{n\sum_{i=1}^{n}x_i^2 -(\sum_{i=1}^{n}x_i)^2}

$$

Now we calculate the last part, 
$$X^TY =\begin{pmatrix}
1&  1 & \dots &1 \\ x_1 & x_2 & \dots & x_n 
\end{pmatrix}\begin{pmatrix}
 y1 \\ y2 \\ \vdots \\ y_n 
\end{pmatrix} = \begin{pmatrix} 
\sum_{i=1}^{n}y_i \\ \sum_{i=1}^{n}x_iy_i
\end{pmatrix} $$

Hence subsitute back, we have 
$$\hat\beta = (X^{T}X)^{-1}X^TY =
\frac{\begin{pmatrix}
\sum_{i=1}^{n}x_i^2  & - \sum_{i=1}^{n}x_i  \\
- \sum_{i=1}^{n}x_i  & n \\
\end{pmatrix}}{n\sum_{i=1}^{n}x_i^2 -(\sum_{i=1}^{n}x_i)^2} \begin{pmatrix} 
\sum_{i=1}^{n}y_i \\ \sum_{i=1}^{n}x_iy_i
\end{pmatrix} 
$$ 
$$=\frac{1}{n\sum_{i=1}^{n}x_i^2}
\begin{pmatrix}
\sum_{i=1}^{n}x_i^2  & - \sum_{i=1}^{n}x_i  \\
- \sum_{i=1}^{n}x_i  & n \\
\end{pmatrix}
\begin{pmatrix} 
\sum_{i=1}^{n}y_i \\ \sum_{i=1}^{n}x_iy_i
\end{pmatrix} 
=\frac{1}{n\sum_{i=1}^{n}x_i^2} 
\begin{pmatrix}
n\bar{y}\sum x_{i}^2-n\bar{x}\sum x_{i}y_{i}\\
-n^2\bar x \bar y + n \sum x_iy_i\\
\end{pmatrix}
$$
Since $\hat{\beta} = (\beta_0,\beta_1)$

We then have $\hat\beta_0 = \bar{y}-\hat{\beta_1}\bar{x}$
and $\hat\beta_1 = \frac{\sum x_iy_i - \bar{x}\bar{y}}{\sum (x_i-\bar{x})^2}$


